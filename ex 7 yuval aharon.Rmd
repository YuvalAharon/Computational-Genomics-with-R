---
title: "Exercise 7"
author: "yuval aharon"
date: "21 12 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(compGenomRData)

# get file paths
fileLGGexp=system.file("extdata",
                      "LGGrnaseq.rds",
                      package="compGenomRData")
fileLGGann=system.file("extdata",
                      "patient2LGGsubtypes.rds",
                      package="compGenomRData")

# gene expression values
gexp=readRDS(fileLGGexp)

# patient annotation
patient=readRDS(fileLGGann)

head(gexp[,1:5])
dim(gexp)

head(patient)
dim(patient)
```


```{r example from the exercise}
install.packages("caret")
library(caret)
boxplot(gexp[,1:50],outline=FALSE,col="cornflowerblue")

par(mfrow=c(1,2))
hist(gexp[,5],xlab="gene expression",main="",border="blue4",
     col="cornflowerblue")

# We add a pseudo count of 1 to avoid log(0).
hist(log10(gexp+1)[,5], xlab="gene expression log scale",main="",
     border="blue4",col="cornflowerblue")

gexp=log10(gexp+1)
boxplot(gexp[,1:50],outline=FALSE,col="cornflowerblue")

# transpose the data set
tgexp <- t(gexp)

# filter predictor variables which have low variation
#The more variables, the slower the algorithms will be generally.

# remove near zero variation for the columns at least
# 85% of the values are the same
# this function creates the filter but doesn't apply it yet
nzv=preProcess(tgexp,method="nzv",uniqueCut = 15)

# apply the filter using "predict" function
# return the filtered dataset and assign it to nzv_tgexp variable.
nzv_tgexp=predict(nzv,tgexp)

dim(tgexp)
dim(nzv_tgexp)

tgexp <- nzv_tgexp

# In addition, we can also choose arbitrary cutoffs for variability. For example, we can choose to take the top 1000 variable predictors.
SDs=apply(tgexp,2,sd )
topPreds=order(SDs,decreasing = TRUE)[1:1000]
tgexp=tgexp[,topPreds]

# We can scale the data.
processCenter=preProcess(tgexp, method = c("center"))
tgexp=predict(processCenter,tgexp)

# create a filter for removing higly correlated variables
# if two variables are highly correlated only one of them
# is removed
corrFilt=preProcess(tgexp, method = "corr",cutoff = 0.9)
tgexp=predict(corrFilt,tgexp)

# For demonstration purposes, we will now introduce NA values in our data
missing_tgexp=tgexp
missing_tgexp[1,1]=NA

# check if there are NA values
anyNA(missing_tgexp) 

#The code below checks which values are NA in the matrix, then runs a column sum and keeps everything where the column sum is equal to 0.
gexpnoNA=missing_tgexp[ , colSums(is.na(missing_tgexp)) == 0]

# We will next try to impute the missing value(s)
mImpute=preProcess(missing_tgexp,method="medianImpute")
imputedGexp=predict(mImpute,missing_tgexp)

library(RANN)
knnImpute=preProcess(missing_tgexp,method="knnImpute")
knnimputedGexp=predict(knnImpute,missing_tgexp)

# Splitting the data - into the test and the training partitions. The reason for this is that we need an independent test we did not train on.
tgexp=merge(patient,tgexp,by="row.names")

# push sample ids back to the row names
rownames(tgexp)=tgexp[,1]
tgexp=tgexp[,-1]

# There are multiple data split strategies. For starters, we will split 30% of the data as the test. This method is the gold standard for testing performance of our model.
set.seed(3031) # set the random number seed for reproducibility
intrain <- createDataPartition(y = tgexp[,1], p= 0.7)[[1]] # get indices for 70% of the data set

# seperate test and training sets
training <- tgexp[intrain,]
testing <- tgexp[-intrain,]

knnFit=knn3(x=training[,-1], # training set
            y=training[,1], # training set class labels
            k=5)
# predictions on the test set
trainPred=predict(knnFit,testing[,-1])

# predictions on the training set
trainPred=predict(knnFit,training[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=training[,1],reference=trainPred)

# predictions on the test set, return class labels
testPred=predict(knnFit,testing[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=testing[,1],reference=testPred)
```

1.
# k-nn Without any transformation or scaling #
```{r}
# gene expression values
gexp=readRDS(fileLGGexp)

tgexp <- t(gexp)
nzv=preProcess(tgexp,method="nzv",uniqueCut = 15)
nzv_tgexp=predict(nzv,tgexp)
dim(tgexp)
dim(nzv_tgexp)
tgexp <- nzv_tgexp

SDs=apply(tgexp,2,sd )
topPreds=order(SDs,decreasing = TRUE)[1:1000]
tgexp=tgexp[,topPreds]
corrFilt=preProcess(tgexp, method = "corr",cutoff = 0.9)
tgexp=predict(corrFilt,tgexp)
anyNA(tgexp)

tgexp=merge(patient,tgexp,by="row.names")
rownames(tgexp)=tgexp[,1]
tgexp=tgexp[,-1]

set.seed(3031) # set the random number seed for reproducibility
intrain <- createDataPartition(y = tgexp[,1], p= 0.7)[[1]] # get indices for 70% of the data set

training <- tgexp[intrain,]
testing <- tgexp[-intrain,]

knnFit=knn3(x=training[,-1], # training set
            y=training[,1], # training set class labels
            k=5)
# predictions on the test set
trainPred=predict(knnFit,testing[,-1])

# predictions on the training set
trainPred=predict(knnFit,training[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=training[,1],reference=trainPred)

# predictions on the test set, return class labels
testPred=predict(knnFit,testing[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=testing[,1],reference=testPred)
```

2.
```{r}
# file path for CpG methylation and age
fileMethAge=system.file("extdata",
                      "CpGmeth2Age.rds",
                      package="compGenomRData")

# read methylation-age table
ameth=readRDS(fileMethAge)
dim(ameth)
summary(ameth[,1:3])

# plot histogram of methylation values
hist(unlist(ameth[,-1]),border="white",
col="cornflowerblue",main="",xlab="methylation values")

#We will remove CpGs that have less than 0.1 standard deviation.
ameth=ameth[,c(TRUE,matrixStats::colSds(as.matrix(ameth[,-1]))>0.1)]
dim(ameth)

set.seed(18)
par(mfrow=c(1,2))
trctrl <- trainControl(method = "none")

# we will now train random forest model
rfregFit <- train(Age~.,data = ameth,method = "ranger",
                  trControl=trctrl,
                  importance="permutation",# calculate importance
                  tuneGrid = data.frame(mtry=50,
                                        min.node.size = 5,
                                        splitrule="variance")
                  )
plot(ameth$Age,rfregFit$finalModel$predictions,
pch=19,xlab="observed Age",
ylab="OOB predicted Age")
mtext(paste("R-squared",
format(rfregFit$finalModel$r.squared,digits=2)))

plot(ameth$Age,(rfregFit$finalModel$predictions-ameth$Age),
pch=18,ylab="residuals (predicted-observed)",
xlab="observed Age",col="blue3")
abline(h=0,col="red4",lty=2)

plot(varImp(rfregFit),top=10)
```



